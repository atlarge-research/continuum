# Deploy 1 VM with all components for the experiment
# For dev/testing purposes only

[infrastructure]
# AWS cloud provider for VMs
provider = aws

# Specification of the VM that will be rented
cloud_nodes = 4
cloud_cores = 4
cloud_memory = 16
cloud_quota = 1.0

# Name of the AWS VM. Its specs should correspond with the options above (for accounting)
# TODO - The control plane (1 of the 4 VMs) doesn't need a GPU
#        Ideally we make it use a non-GPU instance which is cheaper
# Default choice: "g4dn.xlarge" (with 1 GPU per VM)
aws_cloud = "g4dn.xlarge" 

# The AWS region to get the VM from
aws_region = "us-east-2"
aws_zone = "us-east-2a"

# AWS account details -- IAM access keys (can be found in ~/.aws/credentials)
# The access key is ~20 characters, the secret key ~40 characters
aws_access_keys = ""
aws_secret_access_keys = ""

# OS for the VM. This is Amazon Linux 2023 AMI
# For Ubuntu 22.04 on us-east-2:    ami-0986e6d2d2bc905ca
# For AWS Linux on us-east-2:       ami-09b90e09742640522
aws_ami = "ami-0986e6d2d2bc905ca"

# [NOTE] An SSH key is required: ~/.ssh/id_rsa_continuum.pem
# See https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/create-key-pairs.html on how to generate
# Something similar to:
# 1. cd ~/.ssh
# 2. aws ec2 create-key-pair --key-name id_rsa_continuum --key-type rsa --key-format pem \
#       --query "KeyMaterial" --output text > id_rsa_continuum.pem
# 3. chmod 400 id_rsa_continuum.pem

# Enable network emulation with TC between all 4 VMs, with same delays
# Latency is in ms, throughput in mbit. You can enable throughput throttling if you want
# Latency variance uses a normal distribution
# If you don't want a throughput limit, just set a very high value
network_emulation = True
cloud_latency_avg = 25
cloud_latency_var = 5
cloud_throughput = 1000000

[benchmark]
# Resource manager to deploy on the VM
resource_manager = kubernetes

# Force pull Docker containers in case an application container has been updated
docker_pull = True

# Deploy software for observability with the benchmark or resource manager
# Currently only supports deploying on top of Kubernetes
# Options: prometheus, telegraf. Default: None (if left empty/undefined)
observability = telegraf

# The application/use case to deploy on Kubernetes
application = opencraft

# Already run your application on the worker once beforehand (experimental)
# This guarantees the appliation is cached for the run you want to measure afterward
cache_worker = True

# Inform Continuum that this app follows its own scheduling rules
# This ignores any default scheduling of apps on Kubernetes that Continuum uses
custom_scheduling = True

# Whether to use a software pre-installation and post-installation step with applicaiton-specific
# setup, just before and after setting up the resource manager (but always after base_install)
# Currently Kubernetes-specific
app_pre_install = True
app_post_install = True

############################################################################################
# TODO add application-specific parameter below
############################################################################################
sleep_time = 15

# Nodes to schedule the applications on. In this config, you have 4 nodes, from 0 -- 3
# The default value is 0 for all (colocated)
# Also: CPU and memory use (in cores and GB RAM) per application
node_client = 1
cpu_client = 3
memory_client = 10

node_renderer = 2
cpu_renderer = 3
memory_rendere = 10

node_server = 3
cpu_server = 3
memory_server = 10

node_monitor = 0
cpu_monitor = 1
memory_monitor = 5

node_scheduler = 0
cpu_scheduler = 1
memory_scheduler = 5