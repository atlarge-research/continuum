# Deploy 1 VM with all components for the experiment
# For dev/testing purposes only

[infrastructure]
# AWS cloud provider for VMs
provider = aws

############################################################################################
# TODO turn off after testing everything works fine
############################################################################################
infra_only = True

# Specification of the VM that will be rented
cloud_nodes = 1
cloud_cores = 4
cloud_memory = 16
cloud_quota = 1.0

# Name of the AWS VM. Its specs should correspond with the options above (for accounting)
aws_cloud = "g4dn.xlarge"

# The AWS region to get the VM from
aws_region = "us-east-2"
aws_zone = "us-east-2a"

# AWS account details -- IAM access keys (can be found in ~/.aws/credentials)
# The access key is ~20 characters, the secret key ~40 characters
aws_access_keys = ""
aws_secret_access_keys = ""

# OS for the VM. This is Amazon Linux 2023 AMI
# Important: When switching to an Ubuntu OS, need to change VM user name in aws.py:set_ip_names()
#   from ec2-user (for Amazon Linux) to ubuntu (for Ubuntu)
aws_ami = "ami-09b90e09742640522"

# [NOTE] An SSH key is required: ~/.ssh/id_rsa_continuum.pem
# See https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/create-key-pairs.html on how to generate
# Something similar to:
# 1. cd ~/.ssh
# 2. aws ec2 create-key-pair --key-name id_rsa_continuum --key-type rsa --key-format pem \
#       --query "KeyMaterial" --output text > id_rsa_continuum.pem
# 3. chmod 400 id_rsa_continuum.pem

[benchmark]
# Resource manager to deploy on the VM
resource_manager = kubernetes

############################################################################################
# TODO turn off after testing everything works fine
############################################################################################
resource_manager_only = True

# Force pull Docker containers in case an application container has been updated
docker_pull = True

# The application/use case to deploy on Kubernetes
application = opencraft

# The CPU and memory resources per container
############################################################################################
# TODO we will deploy multiple apps on the same VM. Possibly with difference resource usages
############################################################################################
application_worker_cpu = 3.5
application_worker_memory = 10.5

############################################################################################
# TODO this should be ignored. Check if it is actually ignored
############################################################################################
# Number of applications to deploy per cloud/edge worker
# This should work out with the cpu/memory per application and cpu/memory per worker
# So: apps_per_worker * (cpu/mem per app) <= cpu/mem per worker
applications_per_worker = 1 # Options: >= 1. Default: 1 (experimental)

############################################################################################
# TODO add application-specific parameter below
############################################################################################
#
#
#
#

# Already run your application on the worker once beforehand (experimental)
# This guarantees the appliation is cached for the run you want to measure afterward
# Only works for kubernetes-related deployments
############################################################################################
# This should eventually be enabled for this use case. For the entire use case
############################################################################################
cache_worker = False    # Option: True, False (default: False)