---
# This template shows all options for the Continuum framework
# Continuum's configuration has 3 main components:
# - Infrastructure provisioning
# - Software installation and configuration
# - Benchmark execution and data gathering
#
# The infrastructure and software settings can, mostly, be freely combined with each other.
# The benchmarks, however, have strict requirements on what hardware and software can be used
# because benchmark execution and data gathering requires very specific deployments.
#
# The following applies to all configuration settings:
# - Options with a default value are not mandatory
# - Options without a default value are mandatory, unless explicitly mentioned otherwise
#
# [IMPORTANT] For more information:
# - See the /config directory for many example configurations
# - Run your configuration with Continuum - its parser gives detailed feedback
#
base_path: "~" # Location to store generated Continuum files. Default: ~
delete: False # Delete any provisioned infrastructure after benchmark execution. Default: False
# Continuum creates a Docker registry on the host machine to save any container image required
docker_pull: True # Pull container image updates for all required container images
netperf: True # Install and run a netperf network benchmark across all infrastructure
provider_init: # Global provider settings
  qemu: # QEMU virtual machine provider. Only required when qemu is used in any layer
    cpu_pin: True # Pin VM CPU cores to physical host cores (1-on-1 mapping only). Default: False
    prefixIP: 192.168 # IP range "XXX.XXX.___.___" to access VMs. Default: 192.168
    middleIP: 100 # IP range "___.___.XXX.___" to access VMs, with 1 <= XXX <= 254. Default: 100
    middleIP_base: 90 # See middleIP but now for base VMs. Default: 90
    # Use multiple physical machines to deploy QEMU VMs on.
    # Any SSH address will work as long as no SSH flags are needed to execute
    external_physical_machines: user@machine1 # Any valid SSH address. Default: ""
  gcp: # Google Cloud Platform - Compute Engine VMs. Only mandatory if GCP is used in any layer
    region: europe-west4 # Options: Any GCP region
    zone: europe-west4-a # Options: Any GCP zone
    project: continuum-123456 # Options: Any GCP project name
    credentials: "~/.ssh/continuum-123456-12a34b56c78d" # Options: Any GCP service account creds.
  aws: # AWS - EC2 VMs. Only mandatory if AWS is used in any layer
    region: eu-central-1 # Options: Any AWS region
    zone: eu-central-1a # Options: Any AWS zone
layer:
  # Define the infrastructure, software, and benchmarks of the cloud, edge, and endpoint layers
  # You need to define at least 1 layer from cloud/edge/endpoint, and at most all 3. No duplicates.
  - name: cloud # Layer to setup with Continuum. Options: cloud/edge/endpoint
    infrastructure:
      qemu:
        nodes: 2 # Number of VMs to provision. Options: >= 1
        cores: 4 # CPU cores per VM. Options: >= 1
        memory: 4 # Memory per VM in GB. Options: >= 1
        # CPU bandwidth quota (0.25 means CPU is only active 25% of the time)
        quota: 1.0 # Options: 0.1 <=x <= 1.0. Default: 1.0
        storage: # Storage emulation
          read: 0 # Read throughput in MB/s. Options: >= 0. Default: 0 (unlimited)
          write: 0 # Write throughput - see "read"
        network: # Network emulation
          # Emulation presets create symmetrical networks (cloud->edge and back are identical)
          # For emulation preset values, see continuum/infrastructure/network.py
          preset: none # Emulation preset. Options: none, 4G, 5G. Default: none
          # You can customize the emulation of specific links if needed
          # These definitions are unidirectional, so you can emulate asymmetrical networks.
          # Example: In the cloud layer, link destination "edge" will emulate from cloud to edge
          link:
            - destination: cloud # Emulation from layer to destination. Options: cloud/edge/endpoint
              latency_avg: 0 # Average latency (in ms). Options: >= 0.0. Default: 0.0
              latency_var: 0 # Latency variation (in ms). Options: >= 0.0. Default: 0.0
              throughput: 0 # Throughput (in mbit). Options: >= 0.0. Default: 0.0
      gcp:
        nodes: 2 # Number of VMs to provision. Options: >= 1
        cores: 4 # Should be the number of cores the targeted GCP VM has
        memory: 4 # Should be amount of memory the targeted GCP VM has
        name: "e2-medium" # Any GCP Compute Engine VM. Examples: e2-medium/e2-small/e2-micro
      aws:
        nodes: 2 # Number of VMs to provision. Options: >= 1
        cores: 4 # Should be the number of cores the targeted GCP VM has
        memory: 4 # Should be amount of memory the targeted GCP VM has
        name: "t2-medium" # Any AWS EC2 VM. Examples: t2.medium/t2.small/t2.micro
    software: # Can be ommited if you don't want to install any software on the infrastructure
      # Install specific software packages in various configurations on this layer of infrastructure
      # You can select no, one, or multiple packages, but no duplicates
      #
      # Some packages require the installation of other packages. For example, the serverless
      # system "OpenFaaS" can only be deployed on top of the resource manager Kubernetes, and
      # therefore always requires Kubernetes. In this case, you only have to define "OpenFaaS"
      # as a software package to install, and it will automatically install all dependencies,
      # such as Kubernetes, for you. Also mentioning Kubernetes in the config file will most
      # likely result in a crash - try the configuration yourself and see what happens
      # For more information, also check example configurations in the /config folder
      #
      # In general, this software layer should be used to install and configure software packages.
      # The deployment of applications on infrastructure using specific software packages
      # (like Kubernetes) gets handled in the benchmark layer. See comments there for more info
      #
      # ---
      # Deploy Kubernetes - a cloud resource manager
      # Kubernetes can be deployed by itself on any layer
      - name: kubernetes
        observability: False # Deploy Prometheus and Grafana for Kubernetes. Default: False
      # Deploy KubeEdge - a lightweight Kubernetes package
      # KubeEdge should be deployed on the edge. KubeEdge requires Kubernetes, which will
      # automatically be deployed on the cloud. The user doesn't need to configure Kubernetes.
      - name: kubeedge
        observability: False # Deploy Prometheus and Grafana for Kubernetes. Default: False
      # Deploy OpenFaaS - a resource manager on top of Kubernetes
      # Similar to KubeEdge, OpenFaaS relies on Kubernetes. Kubernetes will be deployed on the
      # cloud, and OpenFaaS on top of it. The user doesn't need to configure Kubernetes.
      - name: openfaas
        observability: False # Deploy Prometheus and Grafana for Kubernetes. Default: False
      # Custom version of Kubernetes to benchmark its control path
      # Used for "Columbo"
      - name: kubecontrol
        observability: False # Deploy Prometheus and Grafana for Kubernetes. Default: False
        version: v1.27.0 # Kubernetes version to use. Options: v1.[23-27].0. Default: v1.27.0
benchmark: # Can be ommited if you don't want to benchmark anything
  # The benchmark layer handles the deployment of applications across all infrastructure layers
  # Applications are deployed on infrastructure using software such as a container engine
  # (e.g., Docker) or a resource manager (e.g., Kubernetes).
  #
  # Unlike the infrastructure and software deployment, these benchmarks are fully customized
  # and only support very specific configurations of hardware/software.
  # This is because these benchmarks can be very complex to deploy and execute, and require
  # custom code for parsing the benchmark results
  #
  # We document each benchmark below. For more information, see example configuration files
  # You can deploy no, one, or multiple benchmarks
  # The support of these benchmarks (and combinations) is up to the user however.
  #
  # ---
  # The image classification application consists of a server-side component, running on Kubernetes
  # in the cloud, and a client-side component running as a container on endpoints
  - application: image_classification
    software: kubernetes
    frequency: 5 # No. of datapoints each client sends to the server per second. Options: >= 1
    layer:
      - name: cloud # The server-side uses 1 application per cloud worker
        cores: 1 # No. of CPU cores per app. Options: >= 0.1. Default: layer->cloud->cores - 0.5
        memory: 1 # Alloc. memory (GB) per app. Options: >= 0.1. Default: layer->cloud->memory - 0.5
      - name: endpoint # The client-side uses 1 application per endpoint
        cores: 1 # Options: >= 0.1. Default: layer->endpoint_cores
        memory: 1 # Options: >= 0.1. Default: layer->endpoint_cores
  # Same as on Kubernetes in cloud, but now runs the server on KubeEdge at the edge
  - application: image_classification
    software: kubeedge
    frequency: 5
    layer:
      - name: edge # Edge default values are similar to the Kubernetes in cloud benchmark
        cores: 1
        memory: 1
      - name: endpoint
        cores: 1
        memory: 1
  # Mimics mist computing, where clients on endpoints offload work to more power endpoint devices.
  # To mimic this, we deploy the server-side on edge nodes and do not install any resource manager
  - application: image_classification
    software: none
    frequency: 5
    layer:
      - name: edge
        cores: 1
        memory: 1
      - name: endpoint
        cores: 1
        memory: 1
  # Native deployment, where both client and server run on the same endpoint device
  - application: image_classification
    software: none
    frequency: 5
    layer:
      - name: endpoint
        cores: 1
        memory: 1
  # Same as on Kubernetes in cloud, but now as serverless function on OpenFaaS in the cloud
  - application: image_classification
    software: openfaas
    frequency: 5
    layer:
      - name: cloud
        cores: 1
        memory: 1
      - name: endpoint
        cores: 1
        memory: 1
  # Deploy an application that only sleeps on Kubernetes in the cloud
  # This benchmark evaluates Kubernetes itself, not the application running on it
  - application: empty
    software: kubecontrol
    sleep_time: 30 # How long to sleep in seconds. Options: >= 1
    # The deployment granularity of the application on Kubernetes
    # Example: Split 10 parallel application instances per:
    # - container:  10 containers in 1 pod
    # - pod:        10 pods with 1 container each (default)
    # - file:       10 YAML files, describing 10 total pods. Use 1 kubectl command to deploy
    # - call        Same as file, but now 1 kubectl call per file
    kube_deployment: pod
    cache_worker: False # Pre-run the benchmark to warm-up Kubernetes. Default: False
    layer:
      - name: cloud
        cores: 1 # Number of cores per application deployment
        memory: 1 # Amount of memory in GB per application deployment
        deployments: 25 # Number of applications to deploy per worker node
