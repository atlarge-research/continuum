# This template file shows all configuration options and possible values.
# Do not use inline comments in your config files. 
# This breaks the config parser (so this example doesn't work as well!)
# Mandatory parameters are indicated, and have no default values
# Similarly, parameters with a default value are not mandatory
#-------------------------------------------------
# Infrastructure settings
#-------------------------------------------------
[infrastructure]
# VM provider
provider = qemu         # Options: qemu, gcp, baremetal (mandatory)

# Only do infrastructure deployment, ignore the benchmark
infra_only = False      # Options: True, False. Default: False

# Number of VMs to spawn per tier (should be at least 1 in total)
# If X_nodes > 0, then the corresponding X_cores, X_memory, and X_quota are mandatory
cloud_nodes = 1         # Options: >= 0
edge_nodes = 1          # Options: >= 0
endpoint_nodes = 1      # Options: >= 0

# Number of cores per VM
cloud_cores = 4         # Options: >= 2
edge_cores = 2          # Options: >= 1
endpoint_cores = 1      # Options: >= 1

# Memory in GB per VM
cloud_memory = 16       # Options: >= 1
edge_memory = 8         # Options: >= 1
endpoint_memory = 4     # Options: >= 1

# CPU bandwidth quota (at 0.5 a VM will use a CPU core for half of the time)
cloud_quota = 1.0       # Options: 0.1 <=x <= 1.0
edge_quota = 0.66       # Options: 0.1 <=x <= 1.0
endpoint_quota = 0.33   # Options: 0.1 <=x <= 1.0

# Read and write throughput for disks on cloud, edge, and endpoint VMs
# Values are in MB/s
cloud_read_speed = 0        # Options: >= 0. Default: 0 (unlimited)
edge_read_speed = 0         # Options: >= 0. Default: 0 (unlimited)
endpoint_read_speed = 0     # Options: >= 0. Default: 0 (unlimited)

cloud_write_speed = 0       # Options: >= 0. Default: 0 (unlimited)
edge_write_speed = 0        # Options: >= 0. Default: 0 (unlimited)
endpoint_write_speed = 0    # Options: >= 0. Default: 0 (unlimited)

# Enable cpu core pinning - VM cores will be pinned to physical CPU cores
# Requires total_VM_cores < physical_cores_available (or add more external machines)
cpu_pin = False             # Options: True, False. Default: False

# -----------------------------------
# Enable network emulation (and use default values for wired networking between cloud and edge)
# If network_emulation = False, all parameters until the next ---- line are disabled
network_emulation = True    # Options: True, False. Default: False

# Network preset for wireless communication between endpoint and cloud/edge
wireless_network_preset = 4g    # Options: 4g, 5g. Default: 4g

# Custom network settings, overwrites default values and wireless network presets
# Between cloud nodes
cloud_latency_avg = 0           # Options: >= 0.0. Default: 0
cloud_latency_var = 0           # Options: >= 0.0. Default: 0
cloud_throughput = 1000         # Options: >= 1.0. Default: 1000

# Between edge nodes
edge_latency_avg = 7.5          # Options: >= 0.0. Default: 7.5
edge_latency_var = 2.5          # Options: >= 0.0. Default: 2.5
edge_throughput = 1000          # Options: >= 1.0. Default: 1000

# Between cloud and edge
cloud_edge_latency_avg = 7.5    # Options: >= 0.0. Default: 7.5
cloud_edge_latency_var = 2.5    # Options: >= 0.0. Default: 2.5
cloud_edge_throughput = 1000    # Options: >= 1.0. Default: 1000

# Between cloud and endpoint (4g preset example)
cloud_endpoint_latency_avg = 45 # Options: >= 0.0. Default: 45 (4g)
cloud_endpoint_latency_var = 5  # Options: >= 0.0. Default: 5 (4g)
cloud_endpoint_throughput = 7.21# Options: >= 1.0. Default: 7.21 (4g)

# Between edge and endpoint (4g preset example)
edge_endpoint_latency_avg = 7.5 # Options: >= 0.0. Default: 7.5 (4g)
edge_endpoint_latency_var = 2.5 # Options: >= 0.0. Default: 2.5 (4g)
edge_endpoint_throughput = 7.21 # Options: >= 1.0. Default: 7.21 (4g)
# -----------------------------------

# Use more physical machines than the one you are currently using
external_physical_machines = user@machine1,user@machine2 # Any valid SSH address. Default: []

# Do a netperf network benchmark 
netperf = False                 # Options: True, False. Default: False

# Create a .continuum folder at this location, on every physical machine
# Store all of continuum's files here: Ansible inventory, Libvirt configs, VM images, etc.
# Provide the full path, possibly with ~
base_path = ~                   # Any path on the system where you have permission. Default: ~

# Set IP of VMs
# First half of the IP
prefixIP = 192.168              # XXX.XXX. Default: 192.168

# Set third part for normal VMs and base images
middleIP = 100                  # Any number 1 - 254. Default: 100
middleIP_base = 90              # Any number 1 - 254. Default: 90

# Delete VMs after the framework completed
delete = False                  # Options: True, False. Default: False

# -----------------------------------
# Provider = gcp will use Google Cloud Platform (GCP)
# This requires extra information from the user 

# What machine to use per tier
gcp_cloud = "e2-medium"             # Options: Any GCP machine (mandatory on provider=gcp)
gcp_edge = "e2-small"               # Options: Any GCP machine (mandatory on provider=gcp)
gcp_endpoint = "e2-micro"           # Options: Any GCP machine (mandatory on provider=gcp)

# Compute region
gcp_region = "europe-west4"         # Options: Any GCP region (mandatory on provider=gcp)

# GCP zone within selected region
gcp_zone = "europe-west4-a"         # Options: Any GCP zone (mandatory on provider=gcp)

# GCP project name the user has created
gcp_project = "continuum-123456"    # Options: The user's GCP project name (mandatory on provider=gcp)

# The path to the file strong the user's GCP service account credentials (mandatory on provider=gcp)
gcp_credentials = "~/.ssh/continuum-123456-12a34b56c78d"

# -----------------------------------
# Provider = aws will use Amazon Web Services (AWS)
# This requires extra information from the user 

# What machine to use per tier
aws_cloud = "t2.medium"             # Options: Any AWS machine (mandatory on provider=aws)
aws_edge = "t2.small"               # Options: Any AWS machine (mandatory on provider=aws)
aws_endpoint = "t2.micro"           # Options: Any AWS machine (mandatory on provider=aws)

# Compute region
aws_region = "eu-central-1"         # Options: Any GCP region (mandatory on provider=aws)

# GCP zone within selected region
aws_zone = "eu-central-1a"         # Options: Any GCP zone (mandatory on provider=aws)

#-------------------------------------------------
# Benchmark settings
#-------------------------------------------------
[benchmark]
# This section is optional if infra_only is set

# Resource manager to use for cloud and/or edge (can be ommited for endpoint-only)
resource_manager = kubernetes   # Options: kubernetes (cloud mode), kubeedge (edge mode), mist (no RM edge), none (local processing on endpoints), kubecontrol (experimental) (mandatory)
resource_manager_only = False   # Options: True (only resource manager, no benchmark executed), False (resource manager + benchmark executed). Default: False

# Force docker pull for application updates
docker_pull = False     # Options: True, False. Default: False

# Application to use. Mandatory when resource_manager_only = False, ignore if True
application = image_classification  # Options: image_classification, empty

# CPU (in cores) and memory (in GB) per application for the cloud/edge worker
application_worker_cpu = 3.5       # Options: >= 0.1. Default: cloud_cores - 0.5
application_worker_memory = 10.5   # Options: >= 0.1. Default: (cloud_cores - 0.5) GB

# Same, but now for (endpoint) data producing application
application_endpoint_cpu = 1.0     # Options: >= 0.1. Default: endpoint_cores
application_endpoint_memory = 3.0  # Options: >= 0.1. Default: endpoint_cores GB

# Number of applications to deploy per cloud/edge worker
# This should work out with the cpu/memory per application and cpu/memory per worker
# So: apps_per_worker * (cpu/mem per app) <= cpu/mem per worker
applications_per_worker = 1 # Options: >= 1. Default: 1 (experimental)

# For image classification
# - Data generation frequency in data entities / second
frequency = 5           # Options: >= 1 (mandatory if using this app)

# For empty (experimental)
sleep_time = 60         # Options: >= 1 (mandatory if using this app)

# How to deploy with kube control - split apps on the level of pod, file, container, or call
# Example:      Application with 10 instances (10 parallel codes should run)
# container:    10 containers in 1 pod
# pod:          10 pods with 1 container each (baseline)
# file:         same as pod, but 10 deployment files instead of 1. Uses 1 kubectl command for all.
# call:         Same as file, but 1 kubectl command per file
kube_deployment = pod   # Options: container, pod, file, call

# Only for kubecontrol: What Kubernetes version do you want to run?
# The binary should be already available, and related containers on Dockerhub
# For kubecontrol, options are v1.[23/24/25/26].0.
# For kubernetes, options are v1.27.0
kube_version = v1.27.0  # Default: v1.27.0

# Already run your application on the worker once beforehand (experimental)
# This guarantees the appliation is cached for the run you want to measure afterward
# Only works for kubernetes-related deployments
cache_worker = False    # Option: True, False (default: False)

# Deploy software for observability with the benchmark or resource manager
# Installs Prometheus and Grafana
# Only works with Kubernetes
observability = False   # Option: True, False (default: False)

#-------------------------------------------------
# Execution Model settings
#-------------------------------------------------
[execution_model]
# This section is optional if intra_only is set

model = openfaas  # Options: openfaas (mandatory)
